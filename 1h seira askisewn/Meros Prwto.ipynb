{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Section 1\n",
    "\n",
    "## Efstathios Galanakis 03112172\n",
    "## Ioannis Tzanettis 03112506\n",
    "## Team Number: ???\n",
    "\n",
    "\n",
    "Datasheet Name: Sonar (Number S3)\n",
    "Number of features: ???\n",
    "Number of classes: 2 (Mine, Rock)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This datasheet contains data taken from a sonar signal.\n",
    "First we use read_csv in order to read the datasheet\n",
    "We don't have any missing data so we don't need to use any preprocessing technique for\n",
    "replacing missing values\n",
    "The label of each samples is placed at the last column\n",
    "We need to convert labels string values to int values so we are going to create the following mapping:\n",
    "mapping = {'R' : 0, 'M' : 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 61)\n",
      "(208, 60) (208,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "perc = 0.2\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "datasheet_path='./small/sonar.all-data'\n",
    "datasheet = pd.read_csv(datasheet_path, header=None)\n",
    "\n",
    "labels = datasheet.iloc[:, [-1]]\n",
    "features = datasheet.iloc[:, 0 :-1].values\n",
    "# values = datasheet.values\n",
    "# features = values[:,0:-1]\n",
    "# labels = values[:,-1]\n",
    "mapping = {'R' : 0, 'M' : 1}\n",
    "labels = labels.replace(mapping).values.flatten()\n",
    "print values.shape\n",
    "print features.shape, labels.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Test data are 20% of the total data and the rest is train data\n",
    "So we seperate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 60) (208,)\n",
      "(166, 60) (42, 60) (166,) (42,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features,labels,test_size=0.2)\n",
    "\n",
    "print  features.shape, labels.shape\n",
    "print x_train.shape, x_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We use a default MLP classifier in order to compare with our improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.64      0.58        14\n",
      "          1       0.80      0.71      0.75        28\n",
      "\n",
      "avg / total       0.71      0.69      0.70        42\n",
      "\n",
      "0.690476190476\n"
     ]
    }
   ],
   "source": [
    "default_clf_Mlp = MLPClassifier();\n",
    "default_clf_Mlp.fit(x_train, y_train)\n",
    "results = default_clf_Mlp.predict(x_test)\n",
    "print classification_report(results, y_test)\n",
    "print accuracy_score(results,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Check Dataset\n",
    "We have to check if we have devided correctly our dataset in train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Feature selection\n",
    "Now it's time to create our improved MLP classifier. First we create a VarianceThreshold selector with threshold 5%\n",
    "in order to reduces the number of features. Then we usa a mask (selected_features) in order to keep the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 48)\n",
      "(42, 48)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(0.001)\n",
    "reduced_train_data = selector.fit_transform(x_train)\n",
    "print reduced_train_data.shape\n",
    "\n",
    "selected_features = selector.get_support()\n",
    "reduced_test_data = x_test[:,selected_features]\n",
    "\n",
    "print reduced_test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Scaling features\n",
    "We need to scale our data, so we use a StandardScaler in order to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 48)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import preprocessing as pr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_train_data = scaler.fit_transform(reduced_train_data)\n",
    "scaled_test_data =  scaler.fit_transform(reduced_test_data)\n",
    "\n",
    "print scaled_train_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Cross Validation\n",
    "\n",
    "After data preprocessing, it's time to select the suitable MLP Classifier. We will break our train set into 10 sets\n",
    "and we are going to use 2 metrics: f1_macro and f1_weighted, so we need to make a new scorer which sets the final\n",
    "score as the sum of the previous metrics' score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def my_scorer(estimator, x, y):\n",
    "    prediction = estimator.predict(x)\n",
    "    macro_score = f1_score(y, prediction, average='macro')\n",
    "    weighted_score = f1_score(y, prediction, average='weighted')\n",
    "    return macro_score + weighted_score\n",
    "    \n",
    "k = 10\n",
    "scoring_policy = ['f1_macro', 'f1_weighted']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now it's time to estimate MLP Classifier parameters, using cross validation.\n",
    "First we have to find the best solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs\n",
      "sgd\n",
      "adam\n",
      "lbfgs\n",
      "[1.6720382451226103, 1.2064612045871299, 1.6586339065131632]\n",
      "1.67203824512\n",
      "{'adam': 1.6586339065131632, 'sgd': 1.2064612045871299, 'lbfgs': 1.6720382451226103}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "solver_opts = ['lbfgs', 'sgd', 'adam']\n",
    "\n",
    "f_score=[]\n",
    "leksiko={}\n",
    "for i in range(0,len(solver_opts)):\n",
    "    print solver_opts[i]\n",
    "    clf = MLPClassifier(solver=solver_opts[i])\n",
    "    clf.fit(scaled_train_data,y_train)              \n",
    "    val_scores=cross_val_score(clf,x_train,y_train,cv=k,scoring= my_scorer)\n",
    "    f_score.append( np.mean(val_scores))\n",
    "    leksiko[solver_opts[i]] = np.mean(val_scores)\n",
    "\n",
    "f_solver = solver_opts[np.argmax(f_score)]\n",
    "print f_solver\n",
    "print f_score\n",
    "print np.max(f_score)\n",
    "print leksiko\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Then we have to find the right alpha value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1e-05\n",
      "0.001\n",
      "0.1\n",
      "10.0\n",
      "1000.0\n",
      "10.0\n",
      "[1.6838322852757837, 1.7227497665732963, 1.7094273674382034, 1.7331589626403869, 0.69426847662141788]\n",
      "1.73315896264\n",
      "{1000.0: 0.69426847662141788, 0.001: 1.7227497665732963, 1.0000000000000001e-05: 1.6838322852757837, 10.0: 1.7331589626403869, 0.10000000000000001: 1.7094273674382034}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=10.0, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_opt =  np.logspace(-5, 3, 5)\n",
    "print alpha_opt.size\n",
    "f_score=[]\n",
    "leksiko={}\n",
    "for i in range(0,alpha_opt.size):\n",
    "    print alpha_opt[i]\n",
    "    clf = MLPClassifier(solver=f_solver,alpha=alpha_opt[i])\n",
    "    clf.fit(scaled_train_data,y_train)\n",
    "    val_scores=cross_val_score(clf,scaled_train_data,y_train,cv=k,scoring= my_scorer)\n",
    "    f_score.append( np.mean(val_scores))\n",
    "    leksiko[alpha_opt[i]] = np.mean(val_scores)\n",
    "\n",
    "f_alpha = alpha_opt[np.argmax(f_score)]\n",
    "print f_alpha\n",
    "print f_score\n",
    "print np.max(f_score)\n",
    "\n",
    "print leksiko\n",
    "\n",
    "f_clf =  MLPClassifier(solver=f_solver,alpha=f_alpha)\n",
    "f_clf.fit(scaled_train_data,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now it's time to find the best combination of hidden layers. The range is (100,0) - (0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## PCA\n",
    "we will to use PCA method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 48)\n",
      "(166, 15)\n",
      "(42, 48)\n",
      "(42, 15)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.65      0.65        17\n",
      "          1       0.76      0.76      0.76        25\n",
      "\n",
      "avg / total       0.71      0.71      0.71        42\n",
      "\n",
      "0.714285714286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n = 15\n",
    "pca = PCA(n_components=n)\n",
    "\n",
    "trainPCA = pca.fit_transform(scaled_train_data)\n",
    "testPCA = pca.fit_transform(scaled_test_data)\n",
    "print scaled_train_data.shape\n",
    "print trainPCA.shape\n",
    "\n",
    "print scaled_test_data.shape\n",
    "print testPCA.shape\n",
    "\n",
    "results = f_clf.predict(scaled_test_data)\n",
    "\n",
    "print classification_report(y_test, results)\n",
    "print accuracy_score(results,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "Meros Prwto.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
