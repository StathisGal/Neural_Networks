{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Section 1\n",
    "\n",
    "## Efstathios Galanakis 03112172\n",
    "## Ioannis Tzanettis 03112506\n",
    "## Team Number: ???\n",
    "\n",
    "\n",
    "Datasheet Name: Sonar (Number S3)\n",
    "Number of features: 60\n",
    "Number of classes: 2 (Mine, Rock)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Insert Dataset\n",
    "This datasheet contains data taken from a sonar signal.\n",
    "\n",
    "Taking into account the file sonar.name, which gives us information about the datasheet, there aren't any\n",
    "headers or numbering so when we use the read__csv command, we will add the option header=None. The label of\n",
    "each samples is placed at the last column\n",
    "\n",
    "We use, as we previously said, the command read_csv in order to read the datasheet from the file\n",
    "sonar.all-data. No changes to the file or the datasheet need to be done.\n",
    "\n",
    "The file sonar.names doesn't refer to any missing data so we supposed that there aren't. It says that \n",
    "every feature's value range is between 0.0 and 1.0.\n",
    "\n",
    "We need to convert labels string values to int values so we are going to create the following mapping:\n",
    "mapping = {'R' : 0, 'M' : 1}\n",
    "\n",
    "We have to check if the sample data are unbalanced in case they aren't, to resample them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 61)\n",
      "(208, 60) (208,)\n",
      "0.533653846154\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "datasheet_path='./small/sonar.all-data'\n",
    "datasheet = pd.read_csv(datasheet_path, header=None)\n",
    "\n",
    "labels = datasheet.iloc[:, [-1]]\n",
    "features = datasheet.iloc[:, 0 :-1].values\n",
    "\n",
    "mapping = {'R' : 0, 'M' : 1}\n",
    "labels = labels.replace(mapping).values.flatten()\n",
    "print datasheet.shape\n",
    "print features.shape, labels.shape\n",
    "\n",
    "# check labels freqs\n",
    "label_freq = np.bincount(labels)/np.float64(labels.size)\n",
    "max_rate = np.max(label_freq)\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "if (max_rate > 0.6):\n",
    "    print \"resampling...\"\n",
    "    features, labels = ros.fit_sample(features,labels)\n",
    "\n",
    "print max_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Test data arne 20% of the total data and the rest is train data\n",
    "So we seperate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 60) (208,)\n",
      "(166, 60) (42, 60) (166,) (42,)\n"
     ]
    }
   ],
   "source": [
    "perc = 0.2\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features,labels,test_size=perc)\n",
    "\n",
    "print  features.shape, labels.shape\n",
    "print x_train.shape, x_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Baseline classification\n",
    "After spliting data, it's time to create dummy classifiers and MLP classifier with\n",
    "default parameters, in order to compare with their improved versions. For constant strategy\n",
    "we will predict as M every state which means we have a Mine there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stratified : 0.52380952381\n",
      "most_frequent : 0.571428571429\n",
      "prior : 0.571428571429\n",
      "uniform : 0.452380952381\n",
      "constant : 0.571428571429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default_clf_Mlp : 0.761904761905\n",
      "[[ 9  9]\n",
      " [11 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "state = 1\n",
    "dummy_strategies = [ 'stratified', 'most_frequent', 'prior', 'uniform', 'constant']\n",
    "conf = []\n",
    "macro = []\n",
    "weighted = []\n",
    "precision = []\n",
    "recall = []\n",
    "# dummy_classifiers= {}\n",
    "for i in dummy_strategies:\n",
    "    if (i == 'constant'):\n",
    "        dummy_classifiers=DummyClassifier(strategy=i, constant=state)\n",
    "    else:\n",
    "        dummy_classifiers=DummyClassifier(strategy=i)\n",
    "\n",
    "    dummy_classifiers.fit(x_train,y_train)\n",
    "    results = dummy_classifiers.predict(x_test)\n",
    "    conf.append( confusion_matrix(y_test, results))\n",
    "    macro += [f1_score(y_test, results, average='macro')]\n",
    "    weighted.append(  f1_score(y_test, results, average='weighted'))\n",
    "    precision.append( precision_score(y_test, results))\n",
    "    recall.append(  recall_score(y_test, results))\n",
    "    print dummy_classifiers.strategy, \":\", accuracy_score(results,y_test)\n",
    "    \n",
    "default_clf_Mlp = MLPClassifier();\n",
    "default_clf_Mlp.fit(x_train, y_train)\n",
    "results = default_clf_Mlp.predict(x_test)\n",
    "conf.append( confusion_matrix(y_test, results))\n",
    "macro.append( f1_score(y_test, results, average='macro'))\n",
    "weighted.append(f1_score(y_test, results, average='weighted'))\n",
    "precision.append(  precision_score(y_test, results))\n",
    "recall.append(recall_score(y_test, results))\n",
    "print \"default_clf_Mlp :\",accuracy_score(results,y_test)\n",
    "print conf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Time to plot our metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stratified', 'most_frequent', 'prior', 'uniform', 'constant', 'MLP']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Container object of 6 artists>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "clf_names = dummy_strategies + ['MLP']\n",
    "print clf_names\n",
    "fig = 1\n",
    "plt.figure(fig)\n",
    "plt.suptitle('f1_macro')\n",
    "plt.bar(range(len(macro)),macro,tick_label=clf_names)\n",
    "\n",
    "fig +=1\n",
    "plt.figure(fig)\n",
    "plt.suptitle('f1_weighted')\n",
    "plt.bar(range(len(weighted)),weighted,tick_label=clf_names)\n",
    "\n",
    "fig +=1\n",
    "plt.figure(fig)\n",
    "plt.suptitle('precision')\n",
    "plt.bar(range(len(precision)),precision,tick_label=clf_names)\n",
    "\n",
    "fig +=1\n",
    "plt.figure(fig)\n",
    "plt.suptitle('recall')\n",
    "plt.bar(range(len(recall)),recall,tick_label=clf_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Classifier Optimization\n",
    "## Feature selection\n",
    "\n",
    "Now it's time to create our improved MLP classifier. First we create a VarianceThreshold selector with threshold 5%\n",
    "in order to reduces the number of features. Then we usa a mask (selected_features) in order to keep the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 51)\n",
      "(42, 51)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(0.0001)\n",
    "reduced_train_data = selector.fit_transform(x_train)\n",
    "\n",
    "print reduced_train_data.shape\n",
    "\n",
    "selected_features = selector.get_support()\n",
    "reduced_test_data = x_test[:,selected_features]\n",
    "\n",
    "print reduced_test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Scaling features\n",
    "We need to scale our data, in order to take values in the range [-1, 1]. So we need to\n",
    "check if exist any value bigger than 1.0 or smaller than -1. If there is no such value, no scalling\n",
    "is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 51)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_train_data = scaler.fit_transform(reduced_train_data)\n",
    "scaled_test_data =  scaler.fit_transform(reduced_test_data)\n",
    "\n",
    "print scaled_train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Resampling\n",
    "We need to examine if our dataset is balanced, which means that there is no label, whose rate through the dataset\n",
    "is 1.5 times more frequent than the rest labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.524096385542\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "label_freq = np.bincount(y_train)/np.float64(y_train.size)\n",
    "max_rate = np.max(label_freq)\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "if (max_rate > 0.6):\n",
    "    print \"resampling...\"\n",
    "    scaled_train_data, y_train = ros.fit_sample(scaled_train_data,y_train)\n",
    "\n",
    "print max_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Cross Validation\n",
    "### Create a scorer\n",
    "After data preprocessing, it's time to select the suitable MLP Classifier. We will break our train set into 10 sets\n",
    "and we are going to use 2 metrics: f1_macro and f1_weighted, so we need to make a new scorer which sets the final\n",
    "score as the sum of the previous metrics' score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def my_scorer(estimator, x, y):\n",
    "    prediction = estimator.predict(x)\n",
    "    macro_score = f1_score(y, prediction, average='macro')\n",
    "    weighted_score = f1_score(y, prediction, average='weighted')\n",
    "    prec_score = presicion_score(y, prediction)\n",
    "    rec_score = recall_score(y, prediction)\n",
    "    return macro_score + weighted_score + prec_score + rec_score\n",
    "    \n",
    "k = 10\n",
    "scoring_policy = ['f1_macro', 'f1_weighted']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Parameter estimation\n",
    "Now it's time to estimate MLP Classifier parameters, using cross validation.\n",
    "First we have to find the best solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.743849083493\n",
      "best params: [80, 'identity', 'lbfgs', 'constant', 1.0000000000000001e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.757002016731\n",
      "best params: [80, 'identity', 'lbfgs', 'constant', 0.10000000000000001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805243407836\n",
      "best params: [80, 'logistic', 'lbfgs', 'constant', 1.0000000000000001e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.823840086642\n",
      "best params: [80, 'logistic', 'lbfgs', 'constant', 0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854160441955\n",
      "best params: [80, 'tanh', 'lbfgs', 'constant', 0.10000000000000001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854664643635\n",
      "best params: [160, 'relu', 'lbfgs', 'constant', 0.10000000000000001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854664643635\n",
      "best params: [160, 'relu', 'lbfgs', 'constant', 0.10000000000000001]\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_opts = range(80,170,20)\n",
    "act_opts = ['identity', 'logistic', 'tanh', 'relu']\n",
    "solver_opts = ['lbfgs', 'sgd', 'adam']\n",
    "# max_iter_opts = range(150, 310, 50)\n",
    "max_iter_opts = [200]\n",
    "learn_opts = ['constant', 'invscaling', 'adaptive']\n",
    "alpha_opt =  np.logspace(-5, 3, 5)\n",
    "print len(alpha_opt)\n",
    "\n",
    "best_score = 0\n",
    "best_params = []\n",
    "for tmp_hid_layer in range(0,len(hidden_layer_opts)):\n",
    "    print hidden_layer_opts[tmp_hid_layer]\n",
    "    for tmp_activation in range(0,len(act_opts)):\n",
    "        for tmp_solver in range(0,len(solver_opts)):\n",
    "            for tmp_learn in range(0, len(learn_opts)):\n",
    "                for tmp_alpha in range(0, len(alpha_opt)):\n",
    "                    clf = MLPClassifier(warm_start=True,hidden_layer_sizes=hidden_layer_opts[tmp_hid_layer],activation=act_opts[tmp_activation], \n",
    "                                    solver=solver_opts[tmp_solver], \n",
    "                                    learning_rate=learn_opts[tmp_learn], alpha=alpha_opt[tmp_alpha])\n",
    "                    # clf.fit(scaled_train_data,y_train)\n",
    "                    val_scores=cross_val_score(clf,x_train,y_train,cv=k,scoring= 'f1_weighted', n_jobs=-1)\n",
    "                    f_score= np.mean(val_scores)\n",
    "                    if (f_score > best_score):\n",
    "                        best_score = f_score\n",
    "                        best_params = [hidden_layer_opts[tmp_hid_layer],act_opts[tmp_activation],solver_opts[tmp_solver],learn_opts[tmp_learn],alpha_opt[tmp_alpha]]\n",
    "                        print best_score\n",
    "                        print \"best params:\", best_params\n",
    "                        \n",
    "print best_score\n",
    "print \"best params:\", best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "80\n",
    "0.743849083493\n",
    "best params: [80, 'identity', 'lbfgs', 'constant', 1.0000000000000001e-05]\n",
    "0.757002016731\n",
    "best params: [80, 'identity', 'lbfgs', 'constant', 0.10000000000000001]\n",
    "0.805243407836\n",
    "best params: [80, 'logistic', 'lbfgs', 'constant', 1.0000000000000001e-05]\n",
    "0.823840086642\n",
    "best params: [80, 'logistic', 'lbfgs', 'constant', 0.001]\n",
    "0.854160441955\n",
    "best params: [80, 'tanh', 'lbfgs', 'constant', 0.10000000000000001]\n",
    "100\n",
    "120\n",
    "140\n",
    "160\n",
    "0.854664643635\n",
    "best params: [160, 'relu', 'lbfgs', 'constant', 0.10000000000000001]\n",
    "0.854664643635\n",
    "best params: [160, 'relu', 'lbfgs', 'constant', 0.10000000000000001]\n",
    "\n",
    "\n",
    "Then we have to find the right alpha value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1e-05\n",
      "0.001\n",
      "0.1\n",
      "10.0\n",
      "1000.0\n",
      "0.1\n",
      "[1.5703398532152402, 1.5660143086187976, 1.6302422723475356, 1.3172417228454381, 0.70408616958489101]\n",
      "1.63024227235\n",
      "{1000.0: 0.70408616958489101, 0.001: 1.5660143086187976, 1.0000000000000001e-05: 1.5703398532152402, 10.0: 1.3172417228454381, 0.10000000000000001: 1.6302422723475356}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "f_score=[]\n",
    "leksiko={}\n",
    "for i in range(0,alpha_opt.size):\n",
    "    print alpha_opt[i]\n",
    "    clf = MLPClassifier(solver=f_solver,alpha=alpha_opt[i])\n",
    "    clf.fit(scaled_train_data,y_train)\n",
    "    val_scores=cross_val_score(clf,scaled_train_data,y_train,cv=k,scoring= my_scorer)\n",
    "    f_score.append( np.mean(val_scores))\n",
    "    leksiko[alpha_opt[i]] = np.mean(val_scores)\n",
    "\n",
    "f_alpha = alpha_opt[np.argmax(f_score)]\n",
    "print f_alpha\n",
    "print f_score\n",
    "print np.max(f_score)\n",
    "\n",
    "print leksiko\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now it's time to find the best combination of hidden layers. The range is (50,)-(150,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "140\n",
      "{160: 0.84441705067092054, 130: 0.81438845983737629, 100: 0.8149294049603647, 70: 0.82244479978226115, 200: 0.84441705067092054, 170: 0.83880619094241382, 140: 0.84529265597377046, 110: 0.82182333662209817, 80: 0.81362726025264731, 50: 0.82277503714036215, 180: 0.82603557662381188, 150: 0.82737160901247597, 120: 0.80299073475544058, 90: 0.8316867881573764, 60: 0.83149957598874003, 190: 0.8278758106931482}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f_score=[]\n",
    "leksiko={}\n",
    "\n",
    "    print i\n",
    "    clf = MLPClassifier(solver=f_solver,alpha=f_alpha, hidden_layer_sizes=(i,))\n",
    "    clf.fit(scaled_train_data,y_train)\n",
    "    val_scores=cross_val_score(clf,scaled_train_data,y_train,cv=k,scoring= 'f1_weighted')\n",
    "    f_score.append( np.mean(val_scores))\n",
    "    leksiko[i] = np.mean(val_scores)\n",
    "\n",
    "f_hidden_layers = np.argmax(f_score)*10+50\n",
    "print f_hidden_layers\n",
    "\n",
    "print leksiko\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "After we have found the best parameters, it's time to train our final MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.10000000000000001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(140,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf =  MLPClassifier(solver=f_solver,alpha=f_alpha, hidden_layer_sizes=(140,))\n",
    "clf.fit(scaled_train_data,y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "Meros Prwto.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
